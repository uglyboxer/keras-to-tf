{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "MAX_LEN = 100\n",
    "x = np.random.random((50, ))\n",
    "OOV_VECTOR = x / np.linalg.norm(x)\n",
    "PAD_VECTOR = np.zeros((50, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/oov_vector.p', 'wb') as f:\n",
    "    pickle.dump(OOV_VECTOR, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@inproceedings{pennington2014glove,\\n  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},\\n  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},\\n  title = {GloVe: Global Vectors for Word Representation},\\n  year = {2014},\\n  pages = {1532--1543},\\n  url = {http://www.aclweb.org/anthology/D14-1162},\\n}\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset attribution\n",
    "# https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "'''\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "'''\n",
    "\n",
    "# Glove attribution\n",
    "'''\n",
    "@inproceedings{pennington2014glove,\n",
    "  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},\n",
    "  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},\n",
    "  title = {GloVe: Global Vectors for Word Representation},\n",
    "  year = {2014},\n",
    "  pages = {1532--1543},\n",
    "  url = {http://www.aclweb.org/anthology/D14-1162},\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_data(dir_path):\n",
    "    pos_filepath = os.path.join(dir_path, 'pos/*.txt')\n",
    "    neg_filepath = os.path.join(dir_path, 'neg/*.txt')\n",
    "    \n",
    "    examples = []\n",
    "  \n",
    "    files = glob.glob(pos_filepath)   \n",
    "    for file in files:     \n",
    "        with open(file, 'r') as f:  \n",
    "            examples.append((f.read(), 1))\n",
    "        \n",
    "    files = glob.glob(neg_filepath)   \n",
    "    for file in files:     \n",
    "        with open(file, 'r') as f:  \n",
    "            examples.append((f.read(), 0))\n",
    "        \n",
    "    shuffle(examples)\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_imdb_data('aclImdb/train')\n",
    "test = load_imdb_data('aclImdb/test')  # If you want to use this later as a val or test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train examples: 25000\n",
      "Number of test examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of train examples: {len(train)}')\n",
    "print(f'Number of test examples: {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_input_file=\"data/glove.6B.50d.txt\", word2vec_output_file=\"data/gensim_glove_vectors.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wv_model = KeyedVectors.load_word2vec_format(\"data/gensim_glove_vectors.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vecs(filepath):\n",
    "    vecs = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            vecs[word] = embedding\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model = load_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hey, there, ,, tokenize, me, .]\n",
      "[-0.7001    0.36781   0.34424  -0.42318  -0.046018 -0.66072  -0.33993\n",
      "  0.18271  -0.92863   0.5684   -0.43819   0.70827  -0.47459  -0.079269\n",
      "  1.0187    0.2213    0.43073   0.76719   0.18774  -0.49214  -0.53063\n",
      "  0.56379   0.63571   0.64622   1.2649   -0.82901  -1.3903    0.3749\n",
      "  0.61316  -1.5994    1.3005    0.64347  -0.58004   1.0372   -0.27156\n",
      " -0.43382   0.8554   -0.8967    0.80176  -0.33333  -0.17654  -0.12277\n",
      " -0.70508  -0.28412   0.71149  -0.13487   0.049514 -0.8134    0.34293\n",
      "  1.0381  ]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sample = nlp('Hey there, tokenize me.'.lower())\n",
    "print([x for x in tokenized_sample])\n",
    "print(wv_model[str(tokenized_sample[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(line):\n",
    "    tokens = nlp(line.lower())\n",
    "    return [x for x in tokens if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_pad_sample(example, wv_model):\n",
    "    line, target = example\n",
    "    vectors = []\n",
    "    tokens = preprocess_and_tokenize(line)\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vectors.append(wv_model[str(token)])\n",
    "        except KeyError:\n",
    "            vectors.append(OOV_VECTOR)\n",
    "    pad_len = MAX_LEN - len(vectors)\n",
    "    if pad_len > 0:\n",
    "        vectors.extend([PAD_VECTOR] * pad_len)\n",
    "    return (np.array(vectors[:MAX_LEN]), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "a = ('python or else ' * 32, 1)\n",
    "x, y = vectorize_pad_sample(a, wv_model)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a61575654d4a75a8576765db393f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for example in tqdm(train):\n",
    "    x, y = vectorize_pad_sample(example, wv_model)\n",
    "    X_train.append(x)\n",
    "    y_train.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 20:15:19.688017 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:47: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0905 20:15:19.716189 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:349: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0905 20:15:19.726828 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0905 20:15:19.746971 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3014: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0905 20:15:19.773396 4611966400 deprecation.py:506] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1008: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0905 20:15:19.785884 4611966400 deprecation.py:506] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2683: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Simple model\n",
    "\n",
    "input_shape = X_train[0].shape\n",
    "_input = keras.layers.Input(input_shape)\n",
    "x = keras.layers.Conv1D(25, 3, activation='relu')(_input)\n",
    "x = keras.layers.MaxPooling1D(2)(x)\n",
    "x = keras.layers.Conv1D(50, 3, activation='relu')(x)\n",
    "x = keras.layers.MaxPooling1D(2)(x)\n",
    "x = keras.layers.Conv1D(100, 3, activation='relu')(x)\n",
    "x = keras.layers.GlobalMaxPooling1D()(x)\n",
    "x = keras.layers.Dense(60, activation='relu')(x)\n",
    "x = keras.layers.Dropout(.2)(x)\n",
    "x = keras.layers.Dense(1, activation='sigmoid', name='final_output')(x)\n",
    "\n",
    "model = keras.models.Model(_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'tmp_model_1_weights'\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(weights_file, monitor='val_loss', save_weights_only=True),\n",
    "    keras.callbacks.EarlyStopping(patience=3, monitor='val_loss')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 20:15:19.822947 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/optimizers.py:675: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0905 20:15:19.826447 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2614: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0905 20:15:19.829594 4611966400 deprecation.py:323] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(keras.optimizers.RMSprop(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/engine/training.py:1393: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "W0905 20:15:20.103591 4611966400 deprecation.py:506] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:519: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0905 20:15:20.126255 4611966400 deprecation_wrapper.py:119] From /Users/cole/.local/share/virtualenvs/tf_blog-rq3FUUEN/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:762: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.6165 - acc: 0.6504 - val_loss: 0.5283 - val_acc: 0.7400\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.5291 - acc: 0.7355 - val_loss: 0.4946 - val_acc: 0.7590\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4980 - acc: 0.7575 - val_loss: 0.4824 - val_acc: 0.7624\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4754 - acc: 0.7728 - val_loss: 0.4840 - val_acc: 0.7606\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4626 - acc: 0.7811 - val_loss: 0.4897 - val_acc: 0.7640\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4457 - acc: 0.7890 - val_loss: 0.4810 - val_acc: 0.7646\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4293 - acc: 0.7990 - val_loss: 0.4846 - val_acc: 0.7660\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4127 - acc: 0.8085 - val_loss: 0.5673 - val_acc: 0.7394\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.4014 - acc: 0.8178 - val_loss: 0.5633 - val_acc: 0.7240\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 4s - loss: 0.3876 - acc: 0.8257 - val_loss: 0.5104 - val_acc: 0.7644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x187500438>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split=0.2, batch_size=32, nb_epoch=30, callbacks=callbacks)  # Note old Keras api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(weights_file)\n",
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ('python is the most wonderful language ' * 100, 1)\n",
    "x, y = vectorize_pad_sample(a, wv_model)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8178611]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([x]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_blog",
   "language": "python",
   "name": "tf_blog"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
